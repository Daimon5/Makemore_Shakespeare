{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd67f069-646f-445c-9d4c-62364ea0775a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cd67f069-646f-445c-9d4c-62364ea0775a",
        "outputId": "9f8f6b6a-e022-403c-82aa-7080326beff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0b96e5-4ca7-4769-9e6c-6f323eb1687c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd0b96e5-4ca7-4769-9e6c-6f323eb1687c",
        "outputId": "5e6a20ad-b073-4eda-d284-bf42c8e6e79d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "with open('tiny_shakespeare.txt', 'r', encoding= 'UTF-8') as F:\n",
        "    text = F.read()\n",
        "print(text[:100])\n",
        "chars = sorted(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e1ed80-5622-4334-bb1a-32a6268cb7b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36e1ed80-5622-4334-bb1a-32a6268cb7b1",
        "outputId": "d2f1cb3b-5584-4cc8-c388-2d565c7340a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "print(chars)\n",
        "print(len(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca69d419-92b9-4655-bbaf-99d19e6e00d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca69d419-92b9-4655-bbaf-99d19e6e00d9",
        "outputId": "af0f873c-d8da-45ab-b626-85f66761cf6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ],
      "source": [
        "stoi = {s:i for i, s in enumerate(chars)}\n",
        "print(stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "870ca697-83de-433b-bc50-262d446d68ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "870ca697-83de-433b-bc50-262d446d68ca",
        "outputId": "3af86b51-316b-4355-faee-7f93058452a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ],
      "source": [
        "itos = {i:s for s, i in stoi.items()}\n",
        "print(itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8279672-d875-43aa-bf5d-2f74459735b4",
      "metadata": {
        "id": "e8279672-d875-43aa-bf5d-2f74459735b4"
      },
      "outputs": [],
      "source": [
        "def encode(s:str):\n",
        "    encoded_list = []\n",
        "    for character in s:\n",
        "        encoded_list.append(stoi[character])\n",
        "    return encoded_list\n",
        "\n",
        "def decode(i:int):\n",
        "    decoded_string = \"\"\n",
        "    for integer in i:\n",
        "        decoded_string += itos[integer]\n",
        "    return decoded_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053b7921-5f05-490d-8079-67b0730837bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "053b7921-5f05-490d-8079-67b0730837bb",
        "outputId": "edb24a7a-f327-4ede-cde8-40cf7cce8e7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "decode(encode(\"Hello\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36af18b-44ce-4425-af57-b1ab62d258ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a36af18b-44ce-4425-af57-b1ab62d258ce",
        "outputId": "d952842a-d24c-4b13-9951-5d2cc2b272eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data = torch.tensor(encode(w for w in text))\n",
        "data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efe421c0-1004-4011-8021-c83ca0d6c151",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efe421c0-1004-4011-8021-c83ca0d6c151",
        "outputId": "73d51f3c-e74c-48a3-975c-2e422e8e73ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae88631-da1b-40cd-b8f1-fe58683909ac",
      "metadata": {
        "id": "6ae88631-da1b-40cd-b8f1-fe58683909ac"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87743b20-8a45-4801-b8f9-7fe6e58ff31e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87743b20-8a45-4801-b8f9-7fe6e58ff31e",
        "outputId": "a976f1a9-91b6-4b81-f423-e984aa3dce22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When the input is tensor([18]) the prediction is 47\n",
            "When the input is tensor([18, 47]) the prediction is 56\n",
            "When the input is tensor([18, 47, 56]) the prediction is 57\n",
            "When the input is tensor([18, 47, 56, 57]) the prediction is 58\n",
            "When the input is tensor([18, 47, 56, 57, 58]) the prediction is 1\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1]) the prediction is 15\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1, 15]) the prediction is 47\n",
            "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the prediction is 58\n"
          ]
        }
      ],
      "source": [
        "BLOCK_SIZE = 8\n",
        "BATCH_SIZE = 4\n",
        "X = train_data[:BLOCK_SIZE]\n",
        "y = train_data[1:BLOCK_SIZE+1]\n",
        "for t in range(BLOCK_SIZE):\n",
        "    context = X[:t+1]\n",
        "    print(f\"When the input is {context} the prediction is {y[t]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46c72feb-1ddc-4dfc-bc35-38d858fa262e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46c72feb-1ddc-4dfc-bc35-38d858fa262e",
        "outputId": "d68658f2-b780-4a86-dab2-8e82c9054ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When the input is ['F'] the prediction is i\n",
            "When the input is ['F', 'i'] the prediction is r\n",
            "When the input is ['F', 'i', 'r'] the prediction is s\n",
            "When the input is ['F', 'i', 'r', 's'] the prediction is t\n",
            "When the input is ['F', 'i', 'r', 's', 't'] the prediction is  \n",
            "When the input is ['F', 'i', 'r', 's', 't', ' '] the prediction is C\n",
            "When the input is ['F', 'i', 'r', 's', 't', ' ', 'C'] the prediction is i\n",
            "When the input is ['F', 'i', 'r', 's', 't', ' ', 'C', 'i'] the prediction is t\n"
          ]
        }
      ],
      "source": [
        "context = []\n",
        "for t in range(BLOCK_SIZE):\n",
        "    context.append(itos[X[t].item()])\n",
        "    print(f\"When the input is {context} the prediction is {itos[y[t].item()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03b49de-4c13-49b2-b518-d87e4954e6f5",
      "metadata": {
        "id": "d03b49de-4c13-49b2-b518-d87e4954e6f5"
      },
      "source": [
        "### Building batches of our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6720832a-1805-4e92-b21f-8382d314a7f5",
      "metadata": {
        "id": "6720832a-1805-4e92-b21f-8382d314a7f5"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, batch_size):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(0, len(data) - BLOCK_SIZE, (batch_size,))\n",
        "    # print(ix)\n",
        "    X = torch.stack([data[i : i+BLOCK_SIZE] for i in ix])\n",
        "    y = torch.stack([data[i+1 : i+1+BLOCK_SIZE] for i in ix])\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53919846-143d-4b2d-9f74-79e5009a2312",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53919846-143d-4b2d-9f74-79e5009a2312",
        "outputId": "1421f9a8-6586-423f-a523-aea79df48deb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[53, 52, 45,  1, 50, 47, 60, 43],\n",
              "         [56, 57, 58,  1, 31, 43, 52, 39],\n",
              "         [63,  6,  1, 57, 58, 56, 43, 52],\n",
              "         [ 1, 47, 52, 57, 58, 56, 59, 51]], device='cuda:0'),\n",
              " tensor([[52, 45,  1, 50, 47, 60, 43,  1],\n",
              "         [57, 58,  1, 31, 43, 52, 39, 58],\n",
              "         [ 6,  1, 57, 58, 56, 43, 52, 45],\n",
              "         [47, 52, 57, 58, 56, 59, 51, 43]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "get_batch('train', BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1502ecbf-387b-4b3c-b03d-d36f83650df0",
      "metadata": {
        "id": "1502ecbf-387b-4b3c-b03d-d36f83650df0"
      },
      "outputs": [],
      "source": [
        "Xb, yb = get_batch('train', BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb45eae5-bd8c-41ef-a88b-a7f20ce62f2b",
      "metadata": {
        "id": "eb45eae5-bd8c-41ef-a88b-a7f20ce62f2b"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24363c84-1774-44da-9050-0cca7b19829c",
      "metadata": {
        "id": "24363c84-1774-44da-9050-0cca7b19829c"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets= None):\n",
        "        logits = self.token_embedding_table(x) # B-> Batch dim, T-> Timestep, C-> vocab size (channels)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, max_new_tokens):\n",
        "        # generating characters till max_tokens is hit\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self.forward(x)\n",
        "            logits = logits[:, -1, :] # Bigram model -> only look at the previous character to predict the next\n",
        "            probs = torch.softmax(logits, dim= -1)\n",
        "            x_next = torch.multinomial(probs, num_samples= 1) # (B, 1)\n",
        "            x = torch.cat((x, x_next), dim= 1) # (B, T+1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1631824-1107-4708-ac8b-61c2676cac92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1631824-1107-4708-ac8b-61c2676cac92",
        "outputId": "0c6edb3a-2572-4866-add6-f3387578f16e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 65]),\n",
              " tensor(4.9088, device='cuda:0', grad_fn=<NllLossBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model = BigramLanguageModel(len(chars)).to(device)\n",
        "output, loss = model(Xb, yb)\n",
        "output.shape, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf72e0d5-d740-4e73-af74-016708e25445",
      "metadata": {
        "id": "bf72e0d5-d740-4e73-af74-016708e25445"
      },
      "source": [
        "### Random generation without training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb8b3402-0d84-4b07-9936-7be9aa0163bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb8b3402-0d84-4b07-9936-7be9aa0163bb",
        "outputId": "d52b2bdd-c2bc-4fc0-919c-87645735ddd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "?eyZjb'afwH:aUxEugqFVvz:LJ!wr-mopaA,KtDez$x!e?me$M\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model.generate(context, max_new_tokens= 50)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa0b623b-ee0b-4e70-891e-894b5b19a8de",
      "metadata": {
        "id": "fa0b623b-ee0b-4e70-891e-894b5b19a8de"
      },
      "source": [
        "### Averaging out the losses for stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ace910c-7abf-4407-a40d-9061f8a55260",
      "metadata": {
        "id": "8ace910c-7abf-4407-a40d-9061f8a55260"
      },
      "outputs": [],
      "source": [
        "evaluation_iteration = 100\n",
        "def estimate_losses(model):\n",
        "    with torch.inference_mode():\n",
        "        out = {}\n",
        "        model.eval()\n",
        "        for split in ['train', 'val']:\n",
        "            losses = torch.zeros(evaluation_iteration)\n",
        "            for epoch in range(evaluation_iteration):\n",
        "                X, y = get_batch(split, BATCH_SIZE)\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                logits, loss = model(X, y)\n",
        "                losses[epoch] = loss.item()\n",
        "            out[split] = losses.mean()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2038a8b-6537-4783-ae0c-e59f1ef54e03",
      "metadata": {
        "id": "b2038a8b-6537-4783-ae0c-e59f1ef54e03"
      },
      "source": [
        "## Creating a training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc05195-14c3-4f96-a286-7ef1e33be4cc",
      "metadata": {
        "id": "0bc05195-14c3-4f96-a286-7ef1e33be4cc"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 5000\n",
        "BATCH_SIZE = 32\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6630bf34-4288-4044-a80b-7a9948ff8509",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6630bf34-4288-4044-a80b-7a9948ff8509",
        "outputId": "4d3706ad-ed5e-439e-e787-37b2ded40aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At epoch: 200 {'train': tensor(4.4397), 'val': tensor(4.4352)}\n",
            "At epoch: 400 {'train': tensor(4.2367), 'val': tensor(4.2344)}\n",
            "At epoch: 600 {'train': tensor(4.0487), 'val': tensor(4.0456)}\n",
            "At epoch: 800 {'train': tensor(3.8767), 'val': tensor(3.8681)}\n",
            "At epoch: 1000 {'train': tensor(3.7118), 'val': tensor(3.6949)}\n",
            "At epoch: 1200 {'train': tensor(3.5612), 'val': tensor(3.5623)}\n",
            "At epoch: 1400 {'train': tensor(3.4317), 'val': tensor(3.4340)}\n",
            "At epoch: 1600 {'train': tensor(3.3107), 'val': tensor(3.3222)}\n",
            "At epoch: 1800 {'train': tensor(3.2188), 'val': tensor(3.2101)}\n",
            "At epoch: 2000 {'train': tensor(3.1227), 'val': tensor(3.1219)}\n",
            "At epoch: 2200 {'train': tensor(3.0298), 'val': tensor(3.0321)}\n",
            "At epoch: 2400 {'train': tensor(2.9617), 'val': tensor(2.9609)}\n",
            "At epoch: 2600 {'train': tensor(2.8944), 'val': tensor(2.9048)}\n",
            "At epoch: 2800 {'train': tensor(2.8485), 'val': tensor(2.8518)}\n",
            "At epoch: 3000 {'train': tensor(2.7861), 'val': tensor(2.8065)}\n",
            "At epoch: 3200 {'train': tensor(2.7463), 'val': tensor(2.7654)}\n",
            "At epoch: 3400 {'train': tensor(2.7122), 'val': tensor(2.7291)}\n",
            "At epoch: 3600 {'train': tensor(2.6819), 'val': tensor(2.6928)}\n",
            "At epoch: 3800 {'train': tensor(2.6502), 'val': tensor(2.6716)}\n",
            "At epoch: 4000 {'train': tensor(2.6413), 'val': tensor(2.6439)}\n",
            "At epoch: 4200 {'train': tensor(2.6156), 'val': tensor(2.6240)}\n",
            "At epoch: 4400 {'train': tensor(2.5883), 'val': tensor(2.6050)}\n",
            "At epoch: 4600 {'train': tensor(2.5744), 'val': tensor(2.5804)}\n",
            "At epoch: 4800 {'train': tensor(2.5746), 'val': tensor(2.5805)}\n",
            "At epoch: 5000 {'train': tensor(2.5570), 'val': tensor(2.5673)}\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    Xb, yb = get_batch('train', BATCH_SIZE)\n",
        "    Xb, yb = Xb.to(device), yb.to(device)\n",
        "    logits, loss = model(Xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none= True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"At epoch: {epoch + 1} {estimate_losses(model)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3161342-e27c-48a0-b2b6-cb4ffcd7aa49",
      "metadata": {
        "id": "d3161342-e27c-48a0-b2b6-cb4ffcd7aa49"
      },
      "source": [
        "### Generation after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd5b573-53a2-4c21-839e-3ca0603525e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abd5b573-53a2-4c21-839e-3ca0603525e4",
        "outputId": "18cdf022-8529-49f6-c829-9dababea8807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tisoc-D?\n",
            "Me eedke is ciLose imoto ttheche thaf toured\n",
            "MyZpuspl sie an ler th\n",
            "Whalen withemuriD momT\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model.generate(context, max_new_tokens= 100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db30196-49e3-45af-871c-d07503a137ed",
      "metadata": {
        "id": "4db30196-49e3-45af-871c-d07503a137ed"
      },
      "source": [
        "## Practicing Self attention\n",
        "**Think of attention as a communication mechanism which allows the tokens in a network graph to talk to each other**\n",
        "**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0605b9e4-86de-483b-bb87-3a2210cf4828",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "0605b9e4-86de-483b-bb87-3a2210cf4828"
      },
      "source": [
        "### Performing a single head self attention on x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c343e7c-a18f-4d33-90a7-1ddd38d007cf",
      "metadata": {
        "id": "7c343e7c-a18f-4d33-90a7-1ddd38d007cf"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(17)\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn((B, T, C))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d24fc104-41fb-46ef-b18b-7ad278c6ee9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d24fc104-41fb-46ef-b18b-7ad278c6ee9c",
        "outputId": "1ac6458e-8b15-42cd-f8c9-a184d709e729"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4614, -1.2488,  0.3985,  0.4578, -0.1218,  0.2995, -0.0857, -0.4537],\n",
              "        [-0.1402, -0.1470, -0.8205,  1.2072, -2.1627,  0.8303, -0.8517, -1.7352],\n",
              "        [ 0.1744, -2.4993,  2.4584, -0.1708,  1.8069, -1.1940,  0.6690,  1.3876],\n",
              "        [ 0.5545, -0.1686, -1.7018, -0.0520,  0.3996, -1.1127, -0.4927, -0.1672],\n",
              "        [ 0.3195, -0.6449, -0.7968, -0.4567,  1.0132, -0.1774,  0.2758,  0.6431],\n",
              "        [ 1.1416, -0.4587, -0.8616,  0.8040,  1.1348,  1.0498,  1.5149,  0.4282],\n",
              "        [ 0.0366,  0.5860, -0.0235, -0.8126, -0.2281, -3.5924, -2.0294,  1.1650],\n",
              "        [-0.6182, -1.6939,  2.2444, -0.4055,  2.0177, -1.1242,  1.2002,  1.1739]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "head_size = 16\n",
        "key = nn.Linear(in_features= C, out_features= head_size, bias= False)\n",
        "query = nn.Linear(in_features= C, out_features= head_size, bias= False)\n",
        "value = nn.Linear(in_features= C, out_features= head_size, bias= False)\n",
        "\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "\n",
        "wei = q @ k.permute(0, 2, 1)\n",
        "\n",
        "wei[0] # raw outputs of attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db285f86-8def-4811-997d-5521d662c39e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db285f86-8def-4811-997d-5521d662c39e",
        "outputId": "7261a2b3-26b7-4e3e-d793-8e8ff823bc89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4614,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-0.1402, -0.1470,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.1744, -2.4993,  2.4584,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.5545, -0.1686, -1.7018, -0.0520,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.3195, -0.6449, -0.7968, -0.4567,  1.0132,    -inf,    -inf,    -inf],\n",
              "        [ 1.1416, -0.4587, -0.8616,  0.8040,  1.1348,  1.0498,    -inf,    -inf],\n",
              "        [ 0.0366,  0.5860, -0.0235, -0.8126, -0.2281, -3.5924, -2.0294,    -inf],\n",
              "        [-0.6182, -1.6939,  2.2444, -0.4055,  2.0177, -1.1242,  1.2002,  1.1739]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # in case of sentiment analysis, you want the tokens to look into the future as well\n",
        "wei[0] # not allowing a token to interact with the token after it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1afc67-6e66-4e58-8b78-959d0a86d34b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec1afc67-6e66-4e58-8b78-959d0a86d34b",
        "outputId": "2b72b814-8c4c-419d-b51e-5f4b90c2fdd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5017, 0.4983, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0919, 0.0063, 0.9018, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4683, 0.2272, 0.0490, 0.2554, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2398, 0.0914, 0.0785, 0.1104, 0.4799, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2528, 0.0510, 0.0341, 0.1804, 0.2511, 0.2306, 0.0000, 0.0000],\n",
              "        [0.1991, 0.3449, 0.1875, 0.0852, 0.1528, 0.0053, 0.0252, 0.0000],\n",
              "        [0.0214, 0.0073, 0.3740, 0.0264, 0.2982, 0.0129, 0.1316, 0.1282]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "wei = F.softmax(wei, dim= -1)\n",
        "wei[0] # nromalising the raw values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34033a03-5108-4fc4-8dbf-8f6d3020e98c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34033a03-5108-4fc4-8dbf-8f6d3020e98c",
        "outputId": "147224b1-a8ae-4c1a-91be-a57c614720b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739614af-495c-4f86-8bb6-42a0f179384f",
      "metadata": {
        "id": "739614af-495c-4f86-8bb6-42a0f179384f"
      },
      "source": [
        "### Self attention class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6cb0ba-fb56-4d32-bc66-308202fe5378",
      "metadata": {
        "id": "6e6cb0ba-fb56-4d32-bc66-308202fe5378"
      },
      "outputs": [],
      "source": [
        "BLOCK_SIZE = 96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fec188d-2fa2-4a83-988f-ea7a38c76a24",
      "metadata": {
        "id": "9fec188d-2fa2-4a83-988f-ea7a38c76a24"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(in_features= embedding_dim, out_features= head_size, bias= False)\n",
        "        self.query = nn.Linear(in_features= embedding_dim, out_features= head_size, bias= False)\n",
        "        self.value = nn.Linear(in_features= embedding_dim, out_features= head_size, bias= False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "        self.dropout = nn.Dropout(p= 0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.key(x)\n",
        "        wei = q @ k.permute(0, 2, 1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim= -1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44dde506-0ee1-4af6-bd49-396d3f68590e",
      "metadata": {
        "id": "44dde506-0ee1-4af6-bd49-396d3f68590e"
      },
      "source": [
        "### Multihead attention class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b09ad0a-fcba-4b44-928e-7106628e06e7",
      "metadata": {
        "id": "2b09ad0a-fcba-4b44-928e-7106628e06e7"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "        Computes Multihead Attention on the given inputs\n",
        "\n",
        "        Args:\n",
        "            num_heads : The number of attention heads\n",
        "            head_size : The size of each attention head in the multiheaded attention\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, head_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, embedding_dim) for _ in range(num_heads)])\n",
        "        self.projection = nn.Linear(in_features= embedding_dim, out_features= embedding_dim)\n",
        "        self.dropout = nn.Dropout(p= 0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim= -1)\n",
        "        return self.dropout(self.projection(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d137af-5189-48e6-9819-62db1c3e0180",
      "metadata": {
        "id": "a1d137af-5189-48e6-9819-62db1c3e0180"
      },
      "source": [
        "### FeedForward Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c93545d-fc59-477a-a2a1-bb4d00508413",
      "metadata": {
        "id": "6c93545d-fc59-477a-a2a1-bb4d00508413"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple feed forward network comprising of a linear layer followed by a ReLU and another Linear layer (the projection back into the residual pathway)\n",
        "\n",
        "    Args:\n",
        "        embedding_dim : The embedding dimensions of the input\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(in_features= embedding_dim, out_features= 4 * embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features= 4 * embedding_dim, out_features= embedding_dim),\n",
        "            nn.Dropout(p= 0.2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "655e18e2-5fc7-469f-a9c4-aed6a69a676a",
      "metadata": {
        "id": "655e18e2-5fc7-469f-a9c4-aed6a69a676a"
      },
      "source": [
        "### Transformer Decoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21e210a5-d8d1-430b-a8ec-0755c64c2fe1",
      "metadata": {
        "id": "21e210a5-d8d1-430b-a8ec-0755c64c2fe1"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Communication followed by computation \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super().__init__()\n",
        "        head_size = embedding_dim // num_heads\n",
        "        self.attention = MultiheadAttention(num_heads, head_size, embedding_dim)\n",
        "        self.feed_forward = FeedForward(embedding_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.layer_norm1(x)) # residual connections with layer norm\n",
        "        return x + self.feed_forward(self.layer_norm2(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1be71f2-dc0a-4352-b96b-e0a34adeb96e",
      "metadata": {
        "id": "d1be71f2-dc0a-4352-b96b-e0a34adeb96e"
      },
      "source": [
        "## Transformer architecture for this problem\n",
        "#### With Self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6817094-ba9e-4bbb-949f-9363dd1ed6a4",
      "metadata": {
        "id": "b6817094-ba9e-4bbb-949f-9363dd1ed6a4"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_embedding_table = nn.Embedding(BLOCK_SIZE, embedding_dim)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(embedding_dim, num_heads= 6),\n",
        "            Block(embedding_dim, num_heads= 6),\n",
        "            Block(embedding_dim, num_heads= 6),\n",
        "            Block(embedding_dim, num_heads= 6),\n",
        "            Block(embedding_dim, num_heads= 6),\n",
        "            Block(embedding_dim, num_heads= 6),\n",
        "            nn.LayerNorm(embedding_dim)\n",
        "        )\n",
        "        self.lang_modelling_head = nn.Linear(in_features= embedding_dim, out_features= vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean= 0.0, std= 0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean= 0.0, std= 0.02)\n",
        "\n",
        "    def forward(self, x, targets= None):\n",
        "        B, T = x.shape\n",
        "        token_emdeddings = self.token_embedding_table(x)\n",
        "        pos_embedding = self.positional_embedding_table(torch.arange(0, T, device= device))\n",
        "        X = token_emdeddings + pos_embedding\n",
        "        X = self.blocks(X)\n",
        "        logits = self.lang_modelling_head(X)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, max_new_tokens):\n",
        "        # generating characters till max_tokens is hit\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop x to the last block size tokens\n",
        "            x_contained = x[:, -BLOCK_SIZE:]\n",
        "            logits, loss = self.forward(x_contained)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim= -1)\n",
        "            x_next = torch.multinomial(probs, num_samples= 1) # (B, 1)\n",
        "            x = torch.cat((x, x_next), dim= 1) # (B, T+1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6ad2d1-a246-4351-9332-1f1860915863",
      "metadata": {
        "id": "aa6ad2d1-a246-4351-9332-1f1860915863"
      },
      "outputs": [],
      "source": [
        "model_attention = Transformer(embedding_dim= 384, vocab_size= len(chars)).to(device)\n",
        "EPOCHS = 5000\n",
        "BATCH_SIZE = 32\n",
        "optimizer = torch.optim.AdamW(model_attention.parameters(), lr= 0.0009)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad27aa97-02b9-44d7-ba10-d8133e3ccee9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad27aa97-02b9-44d7-ba10-d8133e3ccee9",
        "outputId": "a5e6106d-0548-42f2-ab02-92fedfc2502a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At epoch: 200 {'train': tensor(2.4975), 'val': tensor(2.4961)}\n",
            "At epoch: 400 {'train': tensor(2.3892), 'val': tensor(2.4084)}\n",
            "At epoch: 600 {'train': tensor(2.1308), 'val': tensor(2.1780)}\n",
            "At epoch: 800 {'train': tensor(1.9799), 'val': tensor(2.0648)}\n",
            "At epoch: 1000 {'train': tensor(1.8475), 'val': tensor(1.9845)}\n",
            "At epoch: 1200 {'train': tensor(1.7649), 'val': tensor(1.9069)}\n",
            "At epoch: 1400 {'train': tensor(1.6955), 'val': tensor(1.8677)}\n",
            "At epoch: 1600 {'train': tensor(1.6398), 'val': tensor(1.8099)}\n",
            "At epoch: 1800 {'train': tensor(1.5943), 'val': tensor(1.7860)}\n",
            "At epoch: 2000 {'train': tensor(1.5649), 'val': tensor(1.7498)}\n",
            "At epoch: 2200 {'train': tensor(1.5285), 'val': tensor(1.7250)}\n",
            "At epoch: 2400 {'train': tensor(1.4997), 'val': tensor(1.6911)}\n",
            "At epoch: 2600 {'train': tensor(1.4742), 'val': tensor(1.6547)}\n",
            "At epoch: 2800 {'train': tensor(1.4487), 'val': tensor(1.6678)}\n",
            "At epoch: 3000 {'train': tensor(1.4390), 'val': tensor(1.6382)}\n",
            "At epoch: 3200 {'train': tensor(1.4270), 'val': tensor(1.6291)}\n",
            "At epoch: 3400 {'train': tensor(1.4081), 'val': tensor(1.6113)}\n",
            "At epoch: 3600 {'train': tensor(1.3868), 'val': tensor(1.5959)}\n",
            "At epoch: 3800 {'train': tensor(1.3793), 'val': tensor(1.6019)}\n",
            "At epoch: 4000 {'train': tensor(1.3731), 'val': tensor(1.5804)}\n",
            "At epoch: 4200 {'train': tensor(1.3712), 'val': tensor(1.5811)}\n",
            "At epoch: 4400 {'train': tensor(1.3509), 'val': tensor(1.5768)}\n",
            "At epoch: 4600 {'train': tensor(1.3469), 'val': tensor(1.5798)}\n",
            "At epoch: 4800 {'train': tensor(1.3326), 'val': tensor(1.5668)}\n",
            "At epoch: 5000 {'train': tensor(1.3313), 'val': tensor(1.5824)}\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model_attention.train()\n",
        "    Xb, yb = get_batch('train', BATCH_SIZE)\n",
        "    Xb, yb = Xb.to(device), yb.to(device)\n",
        "    logits, loss = model_attention(Xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none= True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 200 == 0:\n",
        "        print(f\"At epoch: {epoch + 1} {estimate_losses(model_attention)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generations with different hyperparameters"
      ],
      "metadata": {
        "id": "70q8ix0vpI56"
      },
      "id": "70q8ix0vpI56"
    },
    {
      "cell_type": "code",
      "source": [
        "# Block Size = 96, embedding_dim= 384, num_blocks= 6, num_heads= 6\n",
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model_attention.generate(context, max_new_tokens= 300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh3CNy493aXj",
        "outputId": "5a262475-913c-452d-ef4e-b8ccad47969a"
      },
      "id": "Vh3CNy493aXj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Childrence a charged of graves.\n",
            "\n",
            "YORK:\n",
            "Amen; had etter Julius; like he with pilegrous,\n",
            "What may him shall weep. Come, men! of comes!\n",
            "\n",
            "Nurse:\n",
            "What, as will, I have gone a sign.\n",
            "\n",
            "First Citizen:\n",
            "Ah, in friar, 'twould not made my heart\n",
            "By them far fearful master, my lord, Romeo?\n",
            "Then is an all this plac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block Size = 96, embedding_dim= 384, num_blocks= 6, num_heads= 6\n",
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model_attention.generate(context, max_new_tokens= 300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJplqbwdyfsu",
        "outputId": "6e427cb0-dea9-4dd4-e00c-193d6f53f5b7"
      },
      "id": "vJplqbwdyfsu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "That thou in any else strong what you do?\n",
            "\n",
            "BUSHY:\n",
            "Impatch.\n",
            "\n",
            "LADY ANNE:\n",
            "Thou wilt her to our stiff, good lighters\n",
            "The number by the still wester made a Rutland.\n",
            "What, my good king deeper'd in this heagle:\n",
            "We would to did lay him it, by the speech of street.\n",
            "\n",
            "CLIFFORD:\n",
            "What, as you come?\n",
            "\n",
            "LORD S:\n",
            "Fare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block Size = 64, embedding_dim= 255, num_blocks= 6, num_heads= 5\n",
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model_attention.generate(context, max_new_tokens= 200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8svGYaYsrsbI",
        "outputId": "28c4eada-04f9-4a4a-cb66-d21733ce65f4"
      },
      "id": "8svGYaYsrsbI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Who did death falls dispart bring Camings\n",
            "Well this power: will patient else him from\n",
            "You. Come, Mowbray thy name of the deep to skill.\n",
            "\n",
            "KING RICHARD III:\n",
            "God wonder, here poor let dark on from our re\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block Size = 32, embedding_dim= 255, num_blocks= 5, num_heads= 5\n",
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model_attention.generate(context, max_new_tokens= 200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAJmxiaYpHyn",
        "outputId": "3eed99b3-31a5-4844-b95c-38807dd75bd1"
      },
      "id": "AAJmxiaYpHyn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RICHARD:\n",
            "He madam II soldow bruse to a enter.\n",
            "\n",
            "KING HENRY PARGARET:\n",
            "All, My hand, pull that i' my quicent Hapbandlance?\n",
            "To Sweet provant.\n",
            "\n",
            "WARWICK:\n",
            "Tarry, and me doubtch whom let ring of his my mind.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block Size = 16, embedding_dim= 128, num_blocks= 5, num_heads= 4\n",
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model_attention.generate(context, max_new_tokens= 200)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSMICYDJnTC4",
        "outputId": "424ffb3d-096f-4c83-f4e5-377a3fc3a330"
      },
      "id": "JSMICYDJnTC4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rome.\n",
            "\n",
            "Sweake I will is me; and or pres, my pown!\n",
            "\n",
            "KING RICHARD III:\n",
            "Te'll ne! thiy dukl the wreast, and here speidy; and counce so upal;\n",
            "Nort is Good to peepterse in theu create make.\n",
            "\n",
            "Sece I had duk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "824d2f2b-80ed-4a7f-8da2-0846d101c060",
      "metadata": {
        "id": "824d2f2b-80ed-4a7f-8da2-0846d101c060",
        "outputId": "3a6013b0-6e0b-4180-f89e-8876d33e663f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BARD of I brone;\n",
            "Tha wife to thuis of Come if fredasy mine acious son me to canse dath! I the reppp me\n",
            "And the dewown, with in evely\n",
            "The ocher chine, anio to thee:\n",
            "Here in hinesing.\n",
            "\n",
            "CISING:\n",
            "No, I'll \n"
          ]
        }
      ],
      "source": [
        "# Block Size = 10, embedding_dim= 128, num_blocks= 5, num_heads= 4\n",
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model_attention.generate(context, max_new_tokens= 200)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39e423f-bca0-4cb2-b82b-3c9d5fb1f447",
      "metadata": {
        "id": "d39e423f-bca0-4cb2-b82b-3c9d5fb1f447",
        "outputId": "c64da1bf-ce44-4eef-d1a9-56950c8c2a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FORDOLO:\n",
            "Trowargraids you muscious the lor prove not sthit shoughtan:\n",
            "This your.\n",
            "Lut a mussir your a herep speeevard i nothin reques?\n",
            "Bun upooctist,\n",
            "The bencted thou you sheeld Edward,\n",
            "Vhou he ratough\n"
          ]
        }
      ],
      "source": [
        "# Block Size = 9, embedding_dim= 64, num_blocks= 3, num_heads= 4\n",
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model_attention.generate(context, max_new_tokens= 200)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7032b9f3-0ba6-4a07-8cb3-55da4d2bc967",
      "metadata": {
        "id": "7032b9f3-0ba6-4a07-8cb3-55da4d2bc967",
        "outputId": "8d034555-6968-42b8-bf80-80d1dfec211c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "TOLARD:\n",
            "Penconmided, thou heath; and fore his it, Heng you sughter him listh.\n",
            "\n",
            "SIDUCENIO:\n",
            "Yu you Rom to father wice you I the chen sove and the speast?\n",
            "\n",
            "JUKE VI:\n",
            "Evist\n",
            "As fomm sise.\n",
            "\n",
            "Shown e, a ctosi\n"
          ]
        }
      ],
      "source": [
        "# Block Size = 8, embedding_dim= 64, num_blocks= 3, num_heads= 4\n",
        "context = torch.zeros((1, 1), dtype= torch.long).to(device)\n",
        "print(decode(model_attention.generate(context, max_new_tokens= 200)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6be8ee6-8885-4493-92ec-b03a3bd9579e",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "b6be8ee6-8885-4493-92ec-b03a3bd9579e"
      },
      "source": [
        "## Testing GPU performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8fb7e13-1ecc-465c-875e-4850be71f981",
      "metadata": {
        "id": "b8fb7e13-1ecc-465c-875e-4850be71f981"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "rand1 = torch.rand(10000, 10000).to(device)\n",
        "rand2 = torch.rand(10000, 10000).to(device)\n",
        "\n",
        "np_rand1 = torch.rand(10000, 10000)\n",
        "np_rand2 = torch.rand(10000, 10000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a654dc-c322-46e5-80db-b9f3948ab78a",
      "metadata": {
        "id": "b8a654dc-c322-46e5-80db-b9f3948ab78a"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "rand = rand1 @ rand2\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time for GPU: {elapsed_time:.3f}\")\n",
        "\n",
        "start_time = time.time()\n",
        "rand = np_rand1 @ np_rand2\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Elapsed time for CPU: {elapsed_time:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67fb7230-b678-4a38-9d44-4c97a6d169ee",
      "metadata": {
        "id": "67fb7230-b678-4a38-9d44-4c97a6d169ee"
      },
      "outputs": [],
      "source": [
        "g_cuda = torch.Generator()\n",
        "probabilities = torch.tensor([0.3, 0.7])\n",
        "torch.multinomial(probabilities, num_samples= 5, replacement= True, generator= g_cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b92f0a9-69f3-41e6-9086-9027b185b5ca",
      "metadata": {
        "id": "8b92f0a9-69f3-41e6-9086-9027b185b5ca"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Cuda-GPT",
      "language": "python",
      "name": "cuda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}